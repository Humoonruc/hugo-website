---
title: "文本分析笔记"
subtitle: "jieba 分词包"
author: "黄蒙"
date: "`r Sys.Date()`"
output:
  rticles::ctex:
    toc: yes
    toc_depth: '2'
    df_print: default
    fig_caption: yes
    number_sections: yes
  pdf_document:
    toc: yes
    toc_depth: '2'
  html_document:
    df_print: paged
    fig_caption: yes
    highlight: haddock
    number_sections: yes
    theme: united
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: yes
      smooth_scroll: no
  word_document:
    toc: yes
    toc_depth: '2'
documentclass: ctexart
classoption: hyperref,
---

```{r setup, include = FALSE}

## global options
knitr::opts_chunk$set(
  fig.width = 6, fig.asp = 0.618,
  out.width = "80%", fig.align = "center",
  fig.path = 'Figures/', fig.show = "hold",
  warn = 1, warning = FALSE, message = FALSE, echo = TRUE, 
  comment = '', collapse = F, 
  cache = T, cache.comments = F, autodep = TRUE
)


## use necessary packages
library('pacman')
p_load(tidyverse, lubridate, data.table, # 数据整理
       ggthemes, showtext, gridExtra, # 可视化
       lmtest, plm, orcutt, stats, forecast, zoo, # 统计分析  
       rvest, httr, xml2, # 爬虫
       sqldf, DT, # I/O，其中sqldf包含了RSQLite包
       jiebaR, wordcloud2, webshot, htmlwidgets, tidytext # 文本分析
)
options(sqldf.driver = "SQLite") 

## pdf中图形内部的中文字体设置
pdf.options(family = "GB1")
# 安装字体文件
# font_add('YaHei','MS YaHei.ttf')
windowsFonts(YaHei = windowsFont("Microsoft YaHei"))
showtext_auto(enable = TRUE)
# 包含图的代码块需要fig.showtext = TRUE选项
# ggplot2图形需要在主题中显式指定中文字体才能正常显示图中的中文


## 自定义一般图形主题
mytheme <- theme_economist_white() +
  theme(text = element_text(family = 'YaHei'),
        plot.title = element_text(face = 'bold', size = 14), 
        plot.subtitle = element_text(size = 12),
        plot.caption = element_text(hjust = 0, size = 10, margin = margin(2,0,0,0,'pt')),
        plot.margin = margin(12,10,12,0,'pt'),
        legend.position = 'top',
        legend.justification = 'left',
        legend.margin = margin(4,0,0,0,'pt'),
        legend.key.size = unit(1,'lines'),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 10, margin = margin(0,0,0,0,'pt')),
        axis.text = element_text(size = 10, margin = margin(2,0,2,0,'pt')),
        axis.ticks.length = unit(-4,'pt')
  )

# 自定义柱状图主题
theme_bar <- theme_economist_white() +
  theme(text = element_text(family = 'YaHei'), # 所有的文本字体
        plot.title = element_text(face = 'bold', size = 14), 
        plot.subtitle = element_text(size = 12),
        plot.caption = element_text(hjust = 0, size = 10,
                                    margin = margin(0,0,0,0,'pt')),
        plot.margin = margin(12,0,12,10,'pt'),
        legend.position = 'top',
        legend.justification = 'left',
        legend.margin = margin(4,0,0,0,'pt'),
        legend.key.size = unit(0.7,'lines'),
        legend.title = element_blank(),
        legend.text = element_text(size = 10, margin = margin(0,8,0,4,'pt')),
        axis.text = element_text(size = 10),
        axis.ticks.length = unit(0,'pt') # 不要坐标轴须
  )
```

# 西文文本分析

## 分词 `unnest_tokens()`

> tidytext::unnest_tokens(tbl, output, input, token = "words", format = c("text", "man", "latex", "html", "xml"), to_lower = TRUE, drop = TRUE, collapse = NULL, ...)

第一个参数为数据框，第二个参数为转换后的列名，第三个参数为被转换的列名。

例：

```{r}
text_df <- tibble(line = 1:4, 
                  text = c('I have a dream', 
                           'I want to have a look', 
                           'I love you', 
                           'I will go'))
text_df
word_df <- text_df %>% unnest_tokens(word, text)
word_df
```
效果：
1. 其他列会被保留
2. 标点符号自动删除
3. to_lower参数默认将词转换为小写

## 词频及词频的跨文档比较

```{r}
library(janeaustenr) # 该包中的数据框存有六本小说，一个观测为一行文本

## 读入文本数据，添加行号和章节
original_books <- austen_books() %>% 
  group_by(book) %>% 
  mutate(linenumber = row_number(), # 添加行编号
         chapter = cumsum( # 添加章节号
           str_detect(text, regex("^chapter [\\divxlc]", 
                                  ignore_case = TRUE))
           )
         ) %>% 
  ungroup() 
original_books

## 分词，化为一个观测一词的整洁数据
tidy_books <- original_books %>% unnest_tokens(word, text) 
tidy_books

## 删除停用词
data(stop_words)
stop_words
tidy_books <- tidy_books %>% anti_join(stop_words) # 巧用反连接删除
tidy_books

## 统计词频
tidy_books %>% count(word, sort = TRUE)

## 绘图
tidy_books %>% 
  count(word, sort = TRUE) %>% filter(n > 600) %>% 
  mutate(word = reorder(word, n)) %>% # 按照n固定word的排序，因子化，
  ggplot(aes( word, n)) + geom_col() + xlab(NULL) + coord_flip()
```

## 情感分析

## 关键词

一篇文章中多次出现的高频词未必是文章的关键词，只有那些既比较高频又在其他语料中出现得相对少的词，才是我们要找的关键词。因此，衡量关键词需要在词频的基础上，对每个词分配一个重要性权重。最常见的词（如“的”、“在”、“了”、“我”）给予最小的权重，较常见的词（如“改革”、“创新”）给予较小的权重，较少见的词（如“非典”、“治理整顿”）给予较大的权重。


关键词的一个经典算法是 TF-IDF 算法，$TF-IDF = TF * IDF$。其中，TF (Term Frequency) 为经过标准化的词频，**TF=某个词在文章中出现的次数/文章的总词数** 或 **TF=某个词在文章中出现的次数/出现次数最多的词的出现次数**。IDF (Inverse Document Frequency) 为逆文档频率，与该词出现在语料库中的次数负相关，一般令**IDF = log(语料库文档总数/包含该词的文档数)**。这样，**TF-IDF便与一个词在文档中的出现次数正相关，与该词在整个语料库的分布负相关。**对文档中的每个词计算 TF-IDF 的值，把结果从大到小排序，就得到了这篇文档的关键性排序列表。TF-IDF值最大、排在最前面的几个词，就是这篇文章的关键词。

需要说明的是，tf-idf 是一个基于经验的统计量，缺乏令人信服的理论基础。

## 词之间的关系

### n元词组

### 相关词

## 文档–词项（document-term）矩阵


## 主题建模



# 中文分词技术

## 分词包 jiebaR

jiebaR Shiny APP: <https://qinwf.shinyapps.io/jiebaR-shiny/>  

教程地址：<http://qinwenfeng.com/jiebaR/>

```{r}
# 安装最新版
# library(devtools)
# install_github("qinwf/jiebaRD")
# install_github("qinwf/jiebaR")
# library("jiebaR")
```

结巴分词(jiebaR)，是一款高效的R语言中文分词包，底层使用的是C++，通过Rcpp进行调用很高效。

### worker() 函数

> worker(type = "mix", dict = DICTPATH, hmm = HMMPATH, user = USERPATH, idf = IDFPATH, stop_word = STOPPATH, write = T, qmax = 20, topn = 5, encoding = "UTF-8", detect = T, symbol = F, lines = 1e+05, output = NULL, bylines = F, user_weight = "max")

- type, 引擎类型
- dict, 系统（分词）词典
- hmm, HMM模型路径
- user, 用户词典
- idf, IDF词典
- stop_word, 停止词词典
- write, 是否将文件分词结果写入文件，默认FALSE
- qmax, 最大成词的字符数，默认20个字符
- topn, 关键词数,默认5个
- encoding, 输入文件的编码，默认UTF-8
- detect, 是否编码检查，默认TRUE
- symbol, 是否保留符号，默认FALSE
- lines, 每次读取文件的最大行数，用于控制读取文件的长度。大文件则会分次读取。
- output, 输出路径
- bylines, 按行输出
- user_weight, 用户权重

## 分词

### 分行输出

$bylines = T/F

```{r}
# 新建一个分词器 wk
wk <- worker()
wk
s1 <- segment(c("这是第一行文本。","这是第二行文本。"), wk)
s1

# 分行输出，生成一个list
wk$bylines = TRUE
s2 <- segment(c("这是第一行文本。","这是第二行文本。"), wk)
s2

# 或
wk <- worker(bylines = TRUE)
s3 <- segment(c("这是第一行文本。","这是第二行文本。"), wk)
s3
```

### 保留标点符号

$symbol = T/F^[默认为FALSE] 

```{r}
wk <- worker()
wk$symbol <- T
s1 <- segment(c("Hi，这是第一行文本。"), wk)
s1

# 或
wk <- worker(symbol = T)
s2 <- segment(c("Hi，这是第一行文本。"), wk)
s2
```

### 添加一个作为整体的新词，避免其被分开

#### 方法一

> new_user_word()

```{r}
wk = worker()
segment("这是一个新词", wk)
new_user_word(wk, "这是一个新词", "n") 
# 第三个参数 "n" 代表新词的词性标记
segment("这是一个新词", wk)
```

#### 方法二

自定义user词库，注意:

1. 编码格式必须为utf-8
2. **必须空出首位两行，否则读进来的词会包含文件首位的格式符号**

```{r}
words <- "想学R语言，那就赶紧拿起手机，打开微信，关注公众号《跟着菜鸟一起学R语言》，跟着菜鸟一块飞。"
engine <- worker()
segment(words,engine)

readLines("dictionary.txt") 
engine_user <- worker(user = 'dictionary.txt')
segment(words,engine_user)
```


#### **词性对照表**

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
</style>
<table class="tg">
  <tr>
    <th class="tg-0pky">代码</th>
    <th class="tg-0pky">名称</th>
    <th class="tg-0pky">帮助记忆的诠释</th>
  </tr>
  <tr>
    <td class="tg-0pky">Ag</td>
    <td class="tg-0pky">形语素</td>
    <td class="tg-0pky">形容词性语素。形容词代码为a，语素代码ｇ前面置以A。</td>
  </tr>
  <tr>
    <td class="tg-0pky">a</td>
    <td class="tg-0pky">形容词</td>
    <td class="tg-0pky">取英语形容词adjective的第1个字母。</td>
  </tr>
  <tr>
    <td class="tg-0pky">ad</td>
    <td class="tg-0pky">副形词</td>
    <td class="tg-0pky">直接作状语的形容词。形容词代码a和副词代码d并在一起。</td>
  </tr>
  <tr>
    <td class="tg-0pky">an</td>
    <td class="tg-0pky">名形词</td>
    <td class="tg-0pky">具有名词功能的形容词。形容词代码a和名词代码n并在一起。</td>
  </tr>
  <tr>
    <td class="tg-0pky">b</td>
    <td class="tg-0pky">区别词</td>
    <td class="tg-0pky">取汉字"别"的声母。</td>
  </tr>
  <tr>
    <td class="tg-0pky">c</td>
    <td class="tg-0pky">连词</td>
    <td class="tg-0pky">取英语连词conjunction的第1个字母。</td>
  </tr>
  <tr>
    <td class="tg-0pky">Dg</td>
    <td class="tg-0pky">副语素</td>
    <td class="tg-0pky">副词性语素。副词代码为d，语素代码ｇ前面置以D。</td>
  </tr>
  <tr>
    <td class="tg-0pky">d</td>
    <td class="tg-0pky">副词</td>
    <td class="tg-0pky">取adverb的第2个字母，因其第1个字母已用于形容词。</td>
  </tr>
  <tr>
    <td class="tg-0pky">e</td>
    <td class="tg-0pky">叹词</td>
    <td class="tg-0pky">取英语叹词exclamation的第1个字母。</td>
  </tr>
  <tr>
    <td class="tg-0pky">f</td>
    <td class="tg-0pky">方位词</td>
    <td class="tg-0pky">取汉字"方"的声母。</td>
  </tr>
  <tr>
    <td class="tg-0pky">g</td>
    <td class="tg-0pky">语素</td>
    <td class="tg-0pky">绝大多数语素都能作为合成词的"词根"，取汉字"根"的声母。</td>
  </tr>
  <tr>
    <td class="tg-0pky">h</td>
    <td class="tg-0pky">前接成分</td>
    <td class="tg-0pky">取英语head的第1个字母。</td>
  </tr>
  <tr>
    <td class="tg-0pky">i</td>
    <td class="tg-0pky">成语</td>
    <td class="tg-0pky">取英语成语idiom的第1个字母。</td>
  </tr>
  <tr>
    <td class="tg-0pky">j</td>
    <td class="tg-0pky">简称略语</td>
    <td class="tg-0pky">取汉字"简"的声母。</td>
  </tr>
  <tr>
    <td class="tg-0pky">k</td>
    <td class="tg-0pky">后接成分</td>
    <td class="tg-0pky"></td>
  </tr>
  <tr>
    <td class="tg-0pky">l</td>
    <td class="tg-0pky">习用语</td>
    <td class="tg-0pky">习用语尚未成为成语，有点"临时性"，取"临"的声母。</td>
  </tr>
  <tr>
    <td class="tg-0pky">m</td>
    <td class="tg-0pky">数词</td>
    <td class="tg-0pky">取英语numeral的第3个字母，n，u已有他用。</td>
  </tr>
  <tr>
    <td class="tg-0pky">Ng</td>
    <td class="tg-0pky">名语素</td>
    <td class="tg-0pky">名词性语素。名词代码为n，语素代码ｇ前面置以N。</td>
  </tr>
  <tr>
    <td class="tg-0pky">n</td>
    <td class="tg-0pky">名词</td>
    <td class="tg-0pky">取英语名词noun的第1个字母。</td>
  </tr>
  <tr>
    <td class="tg-0pky">nr</td>
    <td class="tg-0pky">人名</td>
    <td class="tg-0pky">名词代码n和"人(ren)"的声母并在一起。</td>
  </tr>
  <tr>
    <td class="tg-0pky">ns</td>
    <td class="tg-0pky">地名</td>
    <td class="tg-0pky">名词代码n和处所词代码s并在一起。</td>
  </tr>
  <tr>
    <td class="tg-0pky">nt</td>
    <td class="tg-0pky">机构团体</td>
    <td class="tg-0pky">团的声母为t，名词代码n和t并在一起。</td>
  </tr>
  <tr>
    <td class="tg-0pky">nz</td>
    <td class="tg-0pky">其他专名</td>
    <td class="tg-0pky">专的声母的第1个字母为z，名词代码n和z并在一起。</td>
  </tr>
  <tr>
    <td class="tg-0pky">o</td>
    <td class="tg-0pky">拟声词</td>
    <td class="tg-0pky">取英语拟声词onomatopoeia的第1个字母。</td>
  </tr>
  <tr>
    <td class="tg-0pky">p</td>
    <td class="tg-0pky">介词</td>
    <td class="tg-0pky">取英语介词prepositional的第1个字母。</td>
  </tr>
  <tr>
    <td class="tg-0pky">q</td>
    <td class="tg-0pky">量词</td>
    <td class="tg-0pky">取英语quantity的第1个字母。</td>
  </tr>
  <tr>
    <td class="tg-0pky">r</td>
    <td class="tg-0pky">代词</td>
    <td class="tg-0pky">取英语代词pronoun的第2个字母,因p已用于介词。</td>
  </tr>
  <tr>
    <td class="tg-0pky">s</td>
    <td class="tg-0pky">处所词</td>
    <td class="tg-0pky">取英语space的第1个字母。</td>
  </tr>
  <tr>
    <td class="tg-0pky">Tg</td>
    <td class="tg-0pky">时语素</td>
    <td class="tg-0pky">时间词性语素。时间词代码为t,在语素的代码g前面置以T。</td>
  </tr>
  <tr>
    <td class="tg-0pky">t</td>
    <td class="tg-0pky">时间词</td>
    <td class="tg-0pky">取英语time的第1个字母。</td>
  </tr>
  <tr>
    <td class="tg-0pky">u</td>
    <td class="tg-0pky">助词</td>
    <td class="tg-0pky">取英语助词auxiliary 的第2个字母,因a已用于形容词。</td>
  </tr>
  <tr>
    <td class="tg-0pky">Vg</td>
    <td class="tg-0pky">动语素</td>
    <td class="tg-0pky">动词性语素。动词代码为v。在语素的代码g前面置以V。</td>
  </tr>
  <tr>
    <td class="tg-0pky">v</td>
    <td class="tg-0pky">动词</td>
    <td class="tg-0pky">取英语动词verb的第一个字母。</td>
  </tr>
  <tr>
    <td class="tg-0pky">vd</td>
    <td class="tg-0pky">副动词</td>
    <td class="tg-0pky">直接作状语的动词。动词和副词的代码并在一起。</td>
  </tr>
  <tr>
    <td class="tg-0pky">vn</td>
    <td class="tg-0pky">名动词</td>
    <td class="tg-0pky">指具有名词功能的动词。动词和名词的代码并在一起。</td>
  </tr>
  <tr>
    <td class="tg-0pky">w</td>
    <td class="tg-0pky">标点符号</td>
    <td class="tg-0pky"></td>
  </tr>
  <tr>
    <td class="tg-0pky">x</td>
    <td class="tg-0pky">非语素字</td>
    <td class="tg-0pky">非语素字只是一个符号，字母x通常用于代表未知数、符号。</td>
  </tr>
  <tr>
    <td class="tg-0pky">y</td>
    <td class="tg-0pky">语气词</td>
    <td class="tg-0pky">取汉字"语"的声母。</td>
  </tr>
  <tr>
    <td class="tg-0pky">z</td>
    <td class="tg-0pky">状态词</td>
    <td class="tg-0pky">取汉字"状"的声母的前一个字母。</td>
  </tr>
</table>

### 添加停止词

停止词就是分词过程中，我们不需要作为结果的词，像英文的语句中有很多的 'a', 'the', 'or', 'and' 等，中文语言中也有很多，比如“的”，“地”，“得”，“我”，“你”，“他”。这些词因为使用频率过高，会大量出现在一段文本中，对于分词后的结果，在统计词频的时候会增加很多的噪音，所以我们通常都会将这些词进行过滤。

在jiebaR中，过滤停止词有2种方法，一种是通过配置stop_word文件，另一种是使用filter_segment()函数。

#### 方法一

目录下建立一个 stop.txt 文件，内容如下

```{r}
# 配置stop_word文件
readLines("stop.txt")
wk <- worker()
segment("我说，这是一个停止词", wk)
wk <- worker(stop_word = "stop.txt")
segment("我说，这是一个停止词", wk)
```

#### 方法二

```{r}
# 动态调用filter_segment()函数
wk <- worker()
s2 <- segment("这是一个停止词", wk) %>% 
  filter_segment('停止')
s2
```

### 对文件分词

有两种方法，第一种借助于 `readLines()` 可以输出字符串或列表，再借助 `writeBin(charToRaw())` 可以逐行保存，再输出为txt；第二种只能输出txt文件。显然，第一种方法要更加灵活一些。

#### 方法一

```{r}
texts <- readLines("test.txt", encoding = "UTF-8")
wk <- worker(bylines = T) # 不可少
s1 = segment(texts, wk)
s1
# 合并各行分词结果
col <- sapply(s1, function(x) {paste(x, collapse = " ")})
col
# 再合并为一个包含换行符的大字符串
p <- paste(col, collapse = "\n")
p
# 保存并查看
writeBin(charToRaw(p), "result1.txt")
readLines('result1.txt', encoding = "UTF-8")

# 删除，防止程序反复运行，造成append式输出
file.remove("result1.txt")
```

#### 方法二

```{r}
## 第二种
wk$output = "result2.txt"
segment("test.txt", wk)
readLines('result2.txt', encoding = "UTF-8")

file.remove("result2.txt") 
```

## 配置词典

对于分词的结果好坏的关键因素是词典，jiebaR默认有配置标准的词典。对于我们的使用来说，不同行业或不同的文字类型，最好用专门的分词词典。在jiebaR中通过show_dictpath()函数可以查看默认的标准词典。

```{r}
# 查看默认的词库位置
show_dictpath()

# 查看目录中所有文件
dir(show_dictpath())
```

词典目录中，包括了多个文件。

- jieba.dict.utf8, 系统词典文件，最大概率法，utf8编码的
- hmm_model.utf8, 系统词典文件，隐式马尔科夫模型，utf8编码的
- user.dict.utf8, 用户词典文件，utf8编码的
- stop_words.utf8，停止词文件，utf8编码的
- idf.utf8，IDF语料库，utf8编码的
- jieba.dict.zip，jieba.dict.utf8的压缩包
- hmm_model.zip，hmm_model.utf8的压缩包
- idf.zip，idf.utf8的压缩包
- backup.rda，无注释
- model.rda，无注释
- README.md，说明文件

## 词频统计与词云绘制 

### 词频统计

> jiebaR::freq()

输入字符串向量，返回词频统计数据框

```{r}
word_freq <- c(rep("中国", 10), rep("宏观",8), rep("经济", 5), 
               rep('贸易', 4), rep("AI", 3), rep('IT', 5), 
               rep("ICT", 2), rep('5G', 7))
df <- jiebaR::freq(word_freq)
df
```

### 绘制词云

#### wordcloud2 包

官方介绍见<https://www.r-graph-gallery.com/196-the-wordcloud2-library>

> wordcloud2(data, size = 1, minSize = 0, gridSize =  0, fontFamily = 'Segoe UI', fontWeight = 'bold', color = 'random-dark', backgroundColor = "white", minRotation = -pi/4, maxRotation = pi/4, shuffle = TRUE, rotateRatio = 0.4, shape = 'circle', ellipticity = 0.65, widgetsize = NULL, figPath = NULL, hoverFunction = NULL)

- shape: 可以选择词云的形状，有上面代码可知它默认为圆形（circle），它还提供了其他一些参数，cardioid(心形)，star(星形)，diamond（钻石形），triangle-forward（三角形），triangle（三角形），这两个三角形就是倾斜方向不同而已，pentagon(五边形)。
- figPath: 一张黑白图，词云自动填充在黑色区域。figPath为黑白图的路径

```{r}
wordcloud2(df, size = 0.6, fontFamily = "SimHei",
           color = "random-light", backgroundColor = "grey")
wordcloud2(demoFreqC, size = 1, fontFamily = "微软雅黑",
           color = "random-light", backgroundColor = "grey")
```

#### wordcloud2 词云对象的保存

wordcloud2() 生成的图像是用js渲染的，其类型为 html widget，因此不能用一般的方法保存。只能（1）先将其保存为一个 html 文件，（2）再用对 html 截图的方式保存。这两个过程分别要用到`htmlwidgets::saveWidget()`和`webshot::webshot()`。

```{r}
library(webshot)
# webshot::install_phantomjs() 
# 第一次使用webshot包还需要安装额外的东西

# 生成 wordcloud2 图像，my_graph 的类型是 htmlwidget
my_graph <- wordcloud2(demoFreq, size = 1.5)

# htmlwidgets::saveWidget() 可以将 widget 保存为一个 HTML 文件
library("htmlwidgets")
htmlwidgets::saveWidget(my_graph,"tmp.html",selfcontained = F)
```

注意，参数 selfcontained 必须为 F，才能保存一些 external resources, 体现为与tmp.html同级的tmp_files文件夹中的.css和.js文件。有了它们，才能正确完成页面的渲染

```{r}
# webshot::webshot() 用于对一个网页截图
# delay 参数即等待的秒数，等待几秒后再截图，因为可能页面渲染需要时间
webshot("tmp.html", "fig_1.png", delay = 1, 
        vwidth = 1000, vheight = 750)
webshot("tmp.html", "fig_2.png", delay = 2, 
        vwidth = 992, vheight = 744)
webshot("tmp.html", "fig_5.png", delay = 5, 
        vwidth = 1000, vheight = 750)
webshot("tmp.html", "fig_10.png", delay = 10, 
        vwidth = 1000, vheight = 750)
# 可以清楚地看到，等待秒数越多，词云图越精细。
```


## 关键词提取

若不自己写函数定义IDF值，则可以使用默认的IDF表，储存在 jiebaR 包文件夹中。

方法一：

worker()定义提取器，keywords()提取

```{r}
key <- worker("keywords", user = 'dictionary.txt', 
              topn = 10) # topn 即取前几个关键词
keywords("我深入钻研了半年R语言", key)
```

由于字符串中三个词都是只出现了一次，所以TF-IDF的值反映了IDF值的大小。

方法二：

segment() %>% vector_keywords()提取

```{r}
wk <- worker()
s <- segment("我深入钻研了半年R语言", wk)
s
key <- worker("keywords", topn = 10)
vector_keywords(s, key) 
# vector_keywords()将分词结果和关键词提取引擎结合起来
```

可见，**提取关键词时，与单纯分词不同，会自动删除停止词**

# 综合案例：1954-2019年政府工作报告

历年国务院政府工作报告合集网址：
<http://www.gov.cn/guowuyuan/baogao.htm>

## 爬虫抓取

```{r, eval=FALSE}
url <- 'http://www.gov.cn/guowuyuan/baogao.htm'
web <- read_html(url, encoding = "utf-8")

nodes <- web %>% 
  html_nodes("#UCAP-CONTENT a") %>% 
  html_attrs() %>% purrr::map_chr(1) %>% 
  str_sub(1L, -3L) # 各篇报告的网址
n <- length(nodes) # 统计一下共有多少篇


## 自定义抓取函数

# 准备，标题级别对应表
level1 <- data.table(start = c('一、', '二、', '三、', '四、', '五、', '六、', '七、', '八、', '九、'), level = 1)
level2 <- data.table(start = c('（一', '（二', '（三', '（四', '（五', '（六', '（七', '（八', '（九', '（十'), level = 2)
level <- rbind(level1, level2)

# 准备：年份与总理姓名对应表
premier <- tibble(year = 1954:2019, author = '') %>% 
  filter(year < 1961 | year == 1964 | year == 1975 | year > 1977) %>% 
  setDT()
premier[year < 1976, author := '周恩来']
premier[year == 1955, author := '李富春']
premier[year == 1956, author := '李先念']
premier[year == 1958, author := '薄一波']
premier[year == 1960, author := '谭振林']
premier[year %in% c(1978, 1979), author := '华国锋']
premier[year == 1980, author := '姚依林']
premier[year %in% 1981:1987, author := '赵紫阳']
premier[year %in% 1988:1998, author := '李鹏']
premier[year %in% 1999:2003, author := '朱镕基']
premier[year %in% 2004:2013, author := '温家宝']
premier[year %in% 2014:2019, author := '李克强']

# 抓取文献函数，返回一个数据框
crawl <- function(url){
  text <- url %>% read_html(encoding = "utf-8") %>% 
    html_nodes("p") %>% html_text() %>% 
    str_trim() %>% str_remove_all(' ') %>% # 去掉各种空格
    str_replace_all(c("１" = "1", "２" = "2", "３" = "3", 
                      '４' = '4', '５' = '5', '６' = '6',
                      '７' = '7', '８' = '8', '９' = '9',
                      '０' = '0')) # 换为半角数字
  year <- text %>% str_c(collapse = '') %>% 
    str_extract("\\d{4}年\\d{1,2}月\\d{1,2}日") %>% 
    str_sub(1, 4) %>% as.numeric()
  df <- tibble(text = text) %>% mutate(year = year) %>% 
    filter(text != '') %>% 
    mutate(start = str_sub(text, 1, 2)) %>% 
    left_join(level, by = 'start') %>% 
    mutate(level = ifelse(is.na(level), 0, level)) %>% 
    mutate(title = str_c(year, '年国务院政府工作报告')) %>% 
    left_join(premier, by = 'year') %>% 
    select(year, title, author, level, text)
  return(df)
}

## 抓取所有节点背后的政府工作报告，构成一个list
gov_report <- map(nodes, crawl)
```

## 保存进文献数据库

```{r, eval=FALSE}
# 保存1959年以来的（之前的不够规范）国务院政府工作报告
channel <- dbConnect(SQLite(),
  dbname = "C:/Users/humoo/OneDrive/ICT/DataBase/text.db")

dbSendQuery(channel, 'DROP TABLE IF EXISTS gov_report')

# 建表
dbSendQuery(conn = channel, 
"CREATE TABLE gov_report 
(year INTEGER,
title TEXT,
author TEXT,
level INTEGER,
text TEXT)")

# 写入
save2db <- function(db){
  dbWriteTable(channel, "gov_report", db, append = T)
}
map(gov_report, save2db)

# 去掉1957年之前的
dbSendQuery(channel, "DELETE FROM gov_report WHERE year < 1957;")

dbDisconnect(channel) # 断开连接
rm(list = ls())
```

## 高频词和关键词分析

### 高频词

```{r, eval=FALSE}
channel <- dbConnect(SQLite(),
  dbname = "C:/Users/humoo/OneDrive/ICT/DataBase/text.db")
report <- dbReadTable(channel, "gov_report") %>% setDT()

# 报告年份的遍历向量
traversal <- sqldf("SELECT DISTINCT year FROM report")[,1] %>% 
  as.vector()

# 分词引擎
engine <- worker(user = 'user.txt', 
                 stop_word = "stop_words.utf8")

# 自定义词频提取函数
get_tf <- function(year_input) {
  # 1 组合一篇报告为一个长字符串
  string <- report[year == year_input]$text %>% 
    str_c(collapse = '') %>% 
    str_replace_all('\\d',"") # 去掉所有的数字
  # 2 对长字符串分词
  jieba <- segment(string, engine) %>% freq() %>% setDT()
  # 3 计算经过标准化的tf值
  tf <- jieba[,list(year = year_input, words = char, tf = freq/sum(freq))]
  # 4 排序并返回
  tf[order(tf, decreasing = T)] %>% return()
}

# 各年份词频列表
list_tf <- map(traversal, get_tf)
names(list_tf) <- traversal %>% as.character()

# 每年政府工作报告的十大高频词
top10_tf <- function(df){
  df %>% head(10) %>% return()
}
list_tf10 <- map(list_tf, top10_tf)
names(list_tf10) <- traversal %>% as.character()
```

### 关键词

注：idf 的计算公式是一个经验公式，没有多少理论基础。

$idf = lg(48/count))$，刚好可以使得在所有文档中都出现过的词的 idf 为零，从而保证了不会被排入关键词。

```{r, eval=FALSE}
## 合并48篇报告的词库，并去掉了4千多个单字
total_words <- reduce(list_tf,rbind)[str_length(words) > 1]
## 求总词库，配以统计量：每个词在48篇报告中的多少篇中出现过
golssary <- total_words %>% 
  count(words, sort = T, name = 'count') %>% 
  mutate(idf = log(48/count)) # 计算一个词的IDF
# 也可以用sql语句：golssary <- sqldf("SELECT words, COUNT() AS count FROM total_words GROUP BY words ORDER BY count DESC") 


# 每年政府工作报告的三十大关键词
get_idf <- function(year_input){
  list_tf[[year_input %>% as.character()]] %>% 
    left_join(golssary) %>% mutate(tf_idf = tf*idf) %>% 
    arrange(desc(tf_idf)) %>% select(year, words, tf_idf) %>% 
    head(30)
}
list_idf30 <- map(traversal, get_idf)
names(list_idf30) <- traversal %>% as.character()
```

```{r, eval=FALSE}
## 绘制词云图
for (i in traversal) {
  list_idf30[[i %>% as.character()]] %>% 
    select(-year) %>% 
    wordcloud2(size = 0.8, fontFamily = "SimHei", 
               color = "random-light", 
               backgroundColor = "grey") %>% 
    saveWidget("tmp.html",selfcontained = F)
  path <- str_c('Figures/tf-idf_', i, '.png')
  webshot("tmp.html", path, delay = 3, 
          vwidth = 1000, vheight = 750)
}

save(list_tf10, list_idf30, traversal, file = 'C:/Users/humoo/OneDrive/ICT/shinyapp/Government_Report/data/text_analysis.Rdata')
```


```{r, eval=FALSE}

## 将list_tf10和list_idf30数据保存为csv文件，用以进行动态可视化
csv_idf <- reduce(list_idf30, rbind) %>% mutate(type = 'TF-IDF') %>% 
  select(words, type, tf_idf, year) %>% 
  rename(name = words, value = tf_idf, date = year) %>% 
  mutate(value = value * 1000, date = as.Date(str_c(date, '-01-01')))

# 用write_excel_csv()，默认UTF-8编码，避免乱码
write_excel_csv(csv_idf, 'C:/Users/humoo/OneDrive/ICT/shinyapp/Government_Report/data/keywords30.csv')


csv_tf <- reduce(list_tf10, rbind) %>% mutate(type = 'TF') %>% 
  select(words, type, tf, year) %>% 
  rename(name = words, value = tf, date = year) %>% 
  mutate(value = value * 100, date = as.Date(str_c(date, '-01-01')))

write_excel_csv(csv_tf, 'C:/Users/humoo/OneDrive/ICT/shinyapp/Government_Report/data/tf10.csv')
```
