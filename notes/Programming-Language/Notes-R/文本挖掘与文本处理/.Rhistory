## 绘制词云图
for (i in traversal) {
list_idf30[[i %>% as.character()]] %>%
select(-year) %>%
wordcloud2(size = 0.8, fontFamily = "SimHei",
color = "random-light",
backgroundColor = "grey") %>%
saveWidget("tmp.html",selfcontained = F)
path <- str_c('Figures/tf-idf_', i, '.png')
webshot("tmp.html", path, delay = 2,
vwidth = 1000, vheight = 750)
}
channel <- dbConnect(SQLite(),
dbname = "C:/Users/humoo/OneDrive/ICT/DataBase/text.db")
report <- dbReadTable(channel, "gov_report") %>% setDT()
# 报告年份的遍历向量
traversal <- sqldf("SELECT DISTINCT year FROM report")[,1] %>%
as.vector()
# 分词引擎
engine <- worker(user = 'user.txt',
stop_word = "stop_words.utf8")
# 自定义词频提取函数
get_tf <- function(year_input) {
# 1 组合一篇报告为一个长字符串
string <- report[year == year_input]$text %>%
str_c(collapse = '') %>%
str_replace_all('\\d',"") # 去掉所有的数字
# 2 对长字符串分词
jieba <- segment(string, engine) %>% freq() %>% setDT()
# 3 计算tf值
tf <- jieba[,list(year = year_input, words = char, tf = freq/sum(freq))]
# 4 排序并返回
tf[order(tf, decreasing = T)] %>% return()
}
# 各年份词频列表
list_tf <- map(traversal, get_tf)
names(list_tf) <- traversal %>% as.character()
# 每年政府工作报告的十大高频词
top10_tf <- function(df){
df %>% head(10) %>% return()
}
list_tf10 <- map(list_tf, top10_tf)
names(list_tf10) <- traversal %>% as.character()
View(list_tf10)
View(list_tf10[["2019"]])
View(list_tf10[["2019"]])
# 分词引擎
engine <- worker(user = 'user.txt',
stop_word = "stop_words.utf8")
# 自定义词频提取函数
get_tf <- function(year_input) {
# 1 组合一篇报告为一个长字符串
string <- report[year == year_input]$text %>%
str_c(collapse = '') %>%
str_replace_all('\\d',"") # 去掉所有的数字
# 2 对长字符串分词
jieba <- segment(string, engine) %>% freq() %>% setDT()
# 3 计算经过标准化的tf值
tf <- jieba[,list(year = year_input, words = char, tf = freq/sum(freq))]
# 4 排序并返回
tf[order(tf, decreasing = T)] %>% return()
}
# 各年份词频列表
list_tf <- map(traversal, get_tf)
names(list_tf) <- traversal %>% as.character()
# 每年政府工作报告的十大高频词
top10_tf <- function(df){
df %>% head(10) %>% return()
}
list_tf10 <- map(list_tf, top10_tf)
View(list_tf10)
View(list_tf10[["2019"]])
View(list_tf10[["2018"]])
View(list_tf10[["2001"]])
View(list_tf10[["1982"]])
names(list_tf10) <- traversal %>% as.character()
## 合并48篇报告的词库，并去掉了4千多个单字
total_words <- reduce(list_tf,rbind)[str_length(words) > 1]
## 求总词库，配以统计量：每个词在48篇报告中的多少篇中出现过
golssary <- total_words %>%
count(words, sort = T, name = 'count') %>%
mutate(idf = log(48/count)) # 计算一个词的IDF
# 每年政府工作报告的三十大关键词
get_idf <- function(year_input){
list_tf[[year_input %>% as.character()]] %>%
left_join(golssary) %>% mutate(tf_idf = tf*idf) %>%
filter(tf > 0.03) %>%
arrange(desc(tf_idf)) %>% select(year, words, tf_idf) %>%
head(30)
}
list_idf30 <- map(traversal, get_idf)
View(list_idf30)
View(list_idf30[[1]])
channel <- dbConnect(SQLite(),
dbname = "C:/Users/humoo/OneDrive/ICT/DataBase/text.db")
report <- dbReadTable(channel, "gov_report") %>% setDT()
# 报告年份的遍历向量
traversal <- sqldf("SELECT DISTINCT year FROM report")[,1] %>%
as.vector()
channel <- dbConnect(SQLite(),
dbname = "C:/Users/humoo/OneDrive/ICT/DataBase/text.db")
report <- dbReadTable(channel, "gov_report") %>% setDT()
# 报告年份的遍历向量
traversal <- sqldf("SELECT DISTINCT year FROM report")[,1] %>%
as.vector()
# 分词引擎
engine <- worker(user = 'user.txt',
stop_word = "stop_words.utf8")
# 自定义词频提取函数
get_tf <- function(year_input) {
# 1 组合一篇报告为一个长字符串
string <- report[year == year_input]$text %>%
str_c(collapse = '') %>%
str_replace_all('\\d',"") # 去掉所有的数字
# 2 对长字符串分词
jieba <- segment(string, engine) %>% freq() %>% setDT()
# 3 计算经过标准化的tf值
tf <- jieba[,list(year = year_input, words = char, tf = freq/sum(freq))]
# 4 排序并返回
tf[order(tf, decreasing = T)] %>% return()
}
# 各年份词频列表
list_tf <- map(traversal, get_tf)
names(list_tf) <- traversal %>% as.character()
# 每年政府工作报告的十大高频词
top10_tf <- function(df){
df %>% head(10) %>% return()
}
list_tf10 <- map(list_tf, top10_tf)
names(list_tf10) <- traversal %>% as.character()
## 合并48篇报告的词库，并去掉了4千多个单字
total_words <- reduce(list_tf,rbind)[str_length(words) > 1]
## 求总词库，配以统计量：每个词在48篇报告中的多少篇中出现过
golssary <- total_words %>%
count(words, sort = T, name = 'count') %>%
mutate(idf = log(48/count)) # 计算一个词的IDF
# 也可以用sql语句：golssary <- sqldf("SELECT words, COUNT() AS count FROM total_words GROUP BY words ORDER BY count DESC")
# 每年政府工作报告的三十大关键词
get_idf <- function(year_input){
list_tf[[year_input %>% as.character()]] %>%
left_join(golssary) %>% mutate(tf_idf = tf*idf) %>%
filter(tf > 0.03) %>%
arrange(desc(tf_idf)) %>% select(year, words, tf_idf) %>%
head(30)
}
list_idf30 <- map(traversal, get_idf)
names(list_idf30) <- traversal %>% as.character()
View(list_idf30)
View(list_idf30[["2013"]])
View(list_idf30[["2006"]])
View(golssary)
## 合并48篇报告的词库，并去掉了4千多个单字
total_words <- reduce(list_tf,rbind)[str_length(words) > 1]
## 求总词库，配以统计量：每个词在48篇报告中的多少篇中出现过
golssary <- total_words %>%
count(words, sort = T, name = 'count') %>%
mutate(idf = log(48/count)) # 计算一个词的IDF
# 也可以用sql语句：golssary <- sqldf("SELECT words, COUNT() AS count FROM total_words GROUP BY words ORDER BY count DESC")
# 每年政府工作报告的三十大关键词
get_idf <- function(year_input){
list_tf[[year_input %>% as.character()]] %>%
left_join(golssary) %>% mutate(tf_idf = tf*idf) %>%
arrange(desc(tf_idf)) %>% select(year, words, tf_idf) %>%
head(30)
}
list_idf30 <- map(traversal, get_idf)
names(list_idf30) <- traversal %>% as.character()
View(list_idf30)
View(list_idf30[["2016"]])
View(list_idf30[["2010"]])
list_tf10[["2019"]]
list_tf10[["2018"]]
list_idf30[["2019"]]
list_idf30[["2018"]]
## 绘制词云图
for (i in traversal) {
list_idf30[[i %>% as.character()]] %>%
select(-year) %>%
wordcloud2(size = 0.8, fontFamily = "SimHei",
color = "random-light",
backgroundColor = "grey") %>%
saveWidget("tmp.html",selfcontained = F)
path <- str_c('Figures/tf-idf_', i, '.png')
webshot("tmp.html", path, delay = 2,
vwidth = 1000, vheight = 750)
}
save(list_tf10, list_idf30, traversal, file = 'C:/Users/humoo/OneDrive/ICT/shinyapp/Text_analysis/data/text_analysis.Rdata')
channel <- dbConnect(SQLite(),
dbname = "C:/Users/humoo/OneDrive/ICT/DataBase/text.db")
channel <- dbConnect(SQLite(),
dbname = "C:/Users/humoo/OneDrive/ICT/DataBase/text.db")
report <- dbReadTable(channel, "gov_report") %>% setDT()
# 报告年份的遍历向量
traversal <- sqldf("SELECT DISTINCT year FROM report")[,1] %>%
as.vector()
# 分词引擎
engine <- worker(user = 'user.txt',
stop_word = "stop_words.utf8")
# 自定义词频提取函数
get_tf <- function(year_input) {
# 1 组合一篇报告为一个长字符串
string <- report[year == year_input]$text %>%
str_c(collapse = '') %>%
str_replace_all('\\d',"") # 去掉所有的数字
# 2 对长字符串分词
jieba <- segment(string, engine) %>% freq() %>% setDT()
# 3 计算经过标准化的tf值
tf <- jieba[,list(year = year_input, words = char, tf = freq/sum(freq))]
# 4 排序并返回
tf[order(tf, decreasing = T)] %>% return()
}
# 各年份词频列表
list_tf <- map(traversal, get_tf)
names(list_tf) <- traversal %>% as.character()
# 每年政府工作报告的十大高频词
top10_tf <- function(df){
df %>% head(10) %>% return()
}
list_tf10 <- map(list_tf, top10_tf)
names(list_tf10) <- traversal %>% as.character()
## 合并48篇报告的词库，并去掉了4千多个单字
total_words <- reduce(list_tf,rbind)[str_length(words) > 1]
## 求总词库，配以统计量：每个词在48篇报告中的多少篇中出现过
golssary <- total_words %>%
count(words, sort = T, name = 'count') %>%
mutate(idf = log(48/count)) # 计算一个词的IDF
# 也可以用sql语句：golssary <- sqldf("SELECT words, COUNT() AS count FROM total_words GROUP BY words ORDER BY count DESC")
# 每年政府工作报告的三十大关键词
get_idf <- function(year_input){
list_tf[[year_input %>% as.character()]] %>%
left_join(golssary) %>% mutate(tf_idf = tf*idf) %>%
arrange(desc(tf_idf)) %>% select(year, words, tf_idf) %>%
head(30)
}
list_idf30 <- map(traversal, get_idf)
names(list_idf30) <- traversal %>% as.character()
## 绘制词云图
for (i in traversal) {
list_idf30[[i %>% as.character()]] %>%
select(-year) %>%
wordcloud2(size = 0.8, fontFamily = "SimHei",
color = "random-light",
backgroundColor = "grey") %>%
saveWidget("tmp.html",selfcontained = F)
path <- str_c('Figures/tf-idf_', i, '.png')
webshot("tmp.html", path, delay = 3,
vwidth = 1000, vheight = 750)
}
save(list_tf10, list_idf30, traversal, file = 'C:/Users/humoo/OneDrive/ICT/shinyapp/Government_Report/data/text_analysis.Rdata')
rm(list = ls())
library(shiny)
runApp('C:/Users/humoo/OneDrive/ICT/shinyapp/Government_Report')
source('library.R')
library(shiny)
source('library.R')
load("data/text_analysis.Rdata")
library(tidyverse)
library(janeaustenr)
library(tidytext)
install.packages('janeaustenr')
install.packages('tidytext')
library(tidyverse)
library(janeaustenr)
library(tidytext)
austen_books()
?unnest_tokens
## global options
knitr::opts_chunk$set(
fig.width = 6, fig.asp = 0.618,
out.width = "80%", fig.align = "center",
fig.path = 'Figures/', fig.show = "hold",
warn = 1, warning = FALSE, message = FALSE, echo = TRUE,
comment = '', collapse = F,
cache = T, cache.comments = F, autodep = TRUE
)
## use necessary packages
library('pacman')
p_load(tidyverse, lubridate, data.table, # 数据整理
ggthemes, showtext, gridExtra, # 可视化
lmtest, plm, orcutt, stats, forecast, zoo, # 统计分析
rvest, httr, xml2, # 爬虫
sqldf, DT, # I/O，其中sqldf包含了RSQLite包
jiebaR, wordcloud2, webshot, htmlwidgets, tidytext # 文本分析
)
options(sqldf.driver = "SQLite")
## pdf中图形内部的中文字体设置
pdf.options(family = "GB1")
# 安装字体文件
# font_add('YaHei','MS YaHei.ttf')
windowsFonts(YaHei = windowsFont("Microsoft YaHei"))
showtext_auto(enable = TRUE)
# 包含图的代码块需要fig.showtext = TRUE选项
# ggplot2图形需要在主题中显式指定中文字体才能正常显示图中的中文
## 自定义一般图形主题
mytheme <- theme_economist_white() +
theme(text = element_text(family = 'YaHei'),
plot.title = element_text(face = 'bold', size = 14),
plot.subtitle = element_text(size = 12),
plot.caption = element_text(hjust = 0, size = 10, margin = margin(2,0,0,0,'pt')),
plot.margin = margin(12,10,12,0,'pt'),
legend.position = 'top',
legend.justification = 'left',
legend.margin = margin(4,0,0,0,'pt'),
legend.key.size = unit(1,'lines'),
legend.title = element_text(size = 12),
legend.text = element_text(size = 10, margin = margin(0,0,0,0,'pt')),
axis.text = element_text(size = 10, margin = margin(2,0,2,0,'pt')),
axis.ticks.length = unit(-4,'pt')
)
# 自定义柱状图主题
theme_bar <- theme_economist_white() +
theme(text = element_text(family = 'YaHei'), # 所有的文本字体
plot.title = element_text(face = 'bold', size = 14),
plot.subtitle = element_text(size = 12),
plot.caption = element_text(hjust = 0, size = 10,
margin = margin(0,0,0,0,'pt')),
plot.margin = margin(12,0,12,10,'pt'),
legend.position = 'top',
legend.justification = 'left',
legend.margin = margin(4,0,0,0,'pt'),
legend.key.size = unit(0.7,'lines'),
legend.title = element_blank(),
legend.text = element_text(size = 10, margin = margin(0,8,0,4,'pt')),
axis.text = element_text(size = 10),
axis.ticks.length = unit(0,'pt') # 不要坐标轴须
)
text_df <- tibble(line = 1:4, text = c('I have a dream', 'I want to have a look', 'I love you', 'I will go'))
View(text_df)
text_df <- tibble(line = 1:4, text = c('I have a dream', 'I want to have a look', 'I love you', 'I will go'))
text_df
text_df %>% unnest_tokens(word, text)
austen_books()
a <- austen_books()
a
a <- austen_books()
a
original_books <- austen_books() %>% group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^ chapter [\\divxlc]", ignore_case = TRUE)))) %>%
ungroup()
original_books
original_books <- austen_books() %>% group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %>%
ungroup()
original_books
library(tidyverse)
library(data.table)
library(sqldf)
library(htmltools)
library(htmlwidgets)
chapter <- readLines('total_text.txt', encoding = "ANSI") %>%
str_c(collapse = '\n') %>% str_split("#") %>% unlist() %>% tail(80) %>%
map_chr(function(x){str_c('#', x)})
temp <- list()
for (i in 1:80) {
temp[[i]] <- chapter[i] %>% str_split('\n') %>% unlist() %>%
as_tibble() %>% rename(text = value) %>% filter(text != '') %>%
mutate(chapter = i)
}
total <- reduce(temp, rbind) %>% filter(text != '') %>% setDT()
total <- total[, list(chapter, text)]
total[, paragraph := 1:.N]
View(total)
a <- readLines('total_text.txt', encoding = "ANSI")
a <- readLines('total_text.txt', encoding = "ANSI") %>% tibble()
View(a)
a <- readLines('total_text.txt', encoding = "ANSI") %>% tibble(text = .)
View(a)
View(total)
total <- readLines('total_text.txt', encoding = "ANSI") %>%
tibble(text = .) %>% mutate(chapter = cumsum(str_detect(text, '#')))
View(total)
total <- readLines('total_text.txt', encoding = "ANSI") %>%
tibble(text = .) %>%
filter(text != '') %>%
mutate(chapter = cumsum(str_detect(text, '#')))
View(total)
chapter <- readLines('total_text.txt', encoding = "ANSI") %>%
str_c(collapse = '\n') %>% str_split("#") %>% unlist() %>% tail(80) %>%
map_chr(function(x){str_c('#', x)})
temp <- list()
for (i in 1:80) {
temp[[i]] <- chapter[i] %>% str_split('\n') %>% unlist() %>%
as_tibble() %>% rename(text = value) %>% filter(text != '') %>%
mutate(chapter = i)
}
total <- reduce(temp, rbind) %>% filter(text != '') %>% setDT()
total <- total[, list(chapter, text)]
total[, paragraph := 1:.N]
View(total)
total <- readLines('total_text.txt', encoding = "ANSI") %>%
tibble(text = .) %>%
filter(text != '') %>%
mutate(chapter = cumsum(str_detect(text, '#')))
View(total)
total <- readLines('total_text.txt', encoding = "ANSI") %>%
tibble(text = .) %>%
filter(text != '') %>%
mutate(chapter = cumsum(str_detect(text, '#'))) %>%
mutate(paragraph = row_number())
View(total)
total <- readLines('total_text.txt', encoding = "ANSI") %>%
tibble(text = .) %>%
filter(text != '') %>%
mutate(chapter = cumsum(str_detect(text, '#'))) %>%
mutate(paragraph = row_number())
View(total)
## 删除所有注释，得到原文+脂批本
no_annotation_text <- total$text %>% str_replace_all('\\^\\[.+?\\] ','')
no_annotation <- total %>% mutate(text = no_annotation_text) %>%
filter(text != '') %>% setDT()
no_annotation[, paragraph := 1:.N]
View(no_annotation)
# 再删除所有脂批，得到原文本
no_remark_text <- no_annotation$text %>% str_replace_all('`.+?`','')
no_remark <- no_annotation %>% mutate(text = no_remark_text) %>%
filter(text != '　　') %>% setDT()
no_remark[, paragraph := 1:.N]
View(no_remark)
## 删除所有注释，得到原文+脂批本
no_annotation_text <- total$text %>%
str_replace_all('\\^\\[.+?\\] ','') %>%
str_trim()
no_annotation <- total %>% mutate(text = no_annotation_text) %>%
filter(text != '') %>% setDT()
no_annotation[, paragraph := 1:.N]
# 再删除所有脂批，得到原文本
no_remark_text <- no_annotation$text %>%
str_replace_all('`.+?`','') %>%
str_trim()
no_remark <- no_annotation %>% mutate(text = no_remark_text) %>%
filter(text != '　　') %>% setDT()
no_remark[, paragraph := 1:.N]
View(no_remark)
# 再删除所有脂批，得到原文本
no_remark_text <- no_annotation$text %>%
str_replace_all('`.+?`','') %>%
str_trim()
no_remark <- no_annotation %>% mutate(text = no_remark_text) %>%
filter(text != '　　') %>% setDT()
no_remark[, paragraph := 1:.N]
# 保存为文件
save(total, no_annotation, no_remark, file = 'RDoC.RData')
rm(list = ls())
load('RDoC.RData')
View(no_remark)
no_remark$text[2]
no_remark$text[29]
View(no_annotation)
# 再删除所有脂批，得到原文本
no_remark_text <- no_annotation$text %>%
str_replace_all('`.+?`','') %>%
str_trim()
no_remark <- no_annotation %>% mutate(text = no_remark_text) %>%
filter(text != '') %>% setDT()
no_remark[, paragraph := 1:.N]
# 保存为文件
save(total, no_annotation, no_remark, file = 'RDoC.RData')
rm(list = ls())
load('RDoC.RData')
View(no_remark)
original_books
library(janeaustenr) # 该包中的数据框存有六本小说，一个观测为一行文本
original_books <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(), # 添加行编号
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %>% # 添加章节号
ungroup()
original_books
tidy_books <- original_books %>% unnest_tokens(word, text)
tidy_books
## 删除停用词
data(stop_words)
stop_words
stop_words
tidy_books <- tidy_books %>% anti_join(stop_words)
tidy_books
anti_join()
?anti_join()
## 统计词频
tidy_books %>% count(word, sort = TRUE)
tidy_books %>% count(word, sort = TRUE) %>% filter(n > 600) %>% mutate(word = reorder(word, n)) %>% ggplot(aes( word, n)) + geom_col()
tidy_books %>% count(word, sort = TRUE) %>% filter(n > 600)
tidy_books %>% count(word, sort = TRUE) %>% filter(n > 600) %>% mutate(word = reorder(word, n))
tidy_books %>% count(word, sort = TRUE) %>% filter(n > 600) %>% mutate(word = reorder(word, n))
%>% ggplot(aes( word, n)) + geom_col()
tidy_books %>% count(word, sort = TRUE) %>% filter(n > 600) %>% mutate(word = reorder(word, n))
tidy_books %>% count(word, sort = TRUE) %>% filter(n > 600) %>% mutate(word = reorder(word, n))
a <- tidy_books %>% count(word, sort = TRUE) %>% filter(n > 600)
a <- tidy_books %>% count(word, sort = TRUE) %>% filter(n > 600)
b <- tidy_books %>% count(word, sort = TRUE) %>% filter(n > 600) %>% mutate(word = reorder(word, n))
a
b
a
b
b <- tidy_books %>% count(word, sort = TRUE) %>% filter(n > 600) %>% ggplot(aes( word, n)) + geom_col()
tidy_books %>% count(word, sort = TRUE) %>% filter(n > 600) %>% ggplot(aes( word, n)) + geom_col()
## 绘图
tidy_books %>% count(word, sort = TRUE) %>% filter(n > 600) %>% mutate(word = reorder(word, n)) %>% ggplot(aes( word, n)) + geom_col() + xlab(NULL) + coord_flip()
a <- tidy_books %>%
count(word, sort = TRUE) %>% filter(n > 600) %>%
mutate(word = reorder(word, n))
a$word
a
## 绘图
tidy_books %>%
count(word, sort = TRUE) %>% filter(n > 600) %>%
mutate(word = reorder(word, n)) %>% # 按照n固定word的排序，因子化，
ggplot(aes( word, n)) + geom_col() + xlab(NULL) + coord_flip()
?bind_rows()
?drop_na()
