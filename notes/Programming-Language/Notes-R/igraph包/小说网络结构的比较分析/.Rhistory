}
calY <- function (x) {
exp(x) - 3 * exp(-x ^ 2 + x) + 1 %>% return()
}
x <- seq(-2, 2, by = 0.001)
y <- calY(x)
plot(x, y)
plot(x, y)
plot(x, y)
uniroot(calY, c(-1, 0))
uniroot(calY, c(-0, 2))
uniroot(calY, c(-2, 2))
z <- D(quote(sin(x) * cos(x * y)), "x")
z
eval(z, list(x = 1, y = 2))
rnorm(5, 2, 0.5)
?distributions
library(rvest)
library(reticulate)
reticulate::py_run_file("./parseHTML.py")
## global options
knitr::opts_chunk$set(
fig.width = 7, fig.asp = 0.618,
out.width = "90%", fig.align = "center",
fig.path = 'Figures/', fig.show = "hold",
warn = 1, warning = FALSE, message = FALSE,
echo = TRUE, comment = '#', collapse = F,
cache = T, cache.comments = F,
autodep = TRUE
)
## use necessary packages
library('pacman')
p_load(
# data processing
tidyverse, lubridate, data.table,
# visualization
ggthemes, showtext, gridExtra,
# I/O
sqldf,
# web crawler
XML, rvest, httr, reticulate
)
## database engine
options(sqldf.driver = "SQLite")
library(rvest)
library(reticulate)
reticulate::py_run_file("./parseHTML.py")
textData <- py$htmlText
reticulate::py_run_file("./parseHTML.py")
textData <- py$htmlText
textData
textData %>% html_nodes("h1.post-title")
rootNode <- py$htmlText %>% read_html()
rootNode %>% html_nodes("h1.post-title")
rootNode %>% html_nodes("h1.post-title") %>% html_text()
?html_node
rootNode %>% html_nodes(xpath = "h1.post-title") %>% html_text()
rootNode %>% html_nodes(xpath = "h1.post-title") %>% html_text()
rootNode %>% html_nodes("h1.post-title") %>% html_text()
rootNode %>% html_nodes(css = "h1.post-title") %>% html_text()
rootNode %>%
html_nodes(css = "h1.post-title") %>%
html_text()
text <- rootNode %>%
html_nodes(css = "h1.post-title") %>%
html_text()
rootNode %>%
html_nodes(css = "h1.post-title", xpath = NULL) %>%
html_text() # 一路向量化操作
## global options
knitr::opts_chunk$set(
fig.width = 7, fig.asp = 0.618,
out.width = "90%", fig.align = "center",
fig.path = 'Figures/', fig.show = "hold",
warn = 1, warning = FALSE, message = FALSE,
echo = TRUE, comment = '#', collapse = F,
cache = T, cache.comments = F,
autodep = TRUE
)
## use necessary packages
library('pacman')
p_load(
# data processing
tidyverse, lubridate, data.table,
# visualization
ggthemes, showtext, gridExtra,
# I/O
sqldf,
# web crawler
XML, rvest, httr, reticulate
)
## database engine
options(sqldf.driver = "SQLite")
reticulate::repl_python()
import requests
from bs4 import BeautifulSoup
r = requests.get('https://python123.io/ws/demo.html')
htmlText = r.text
quit
htmlTextData <- py$htmlText
htmlTextObject <- XML::htmlParse(htmlTextData)
htmlTextObject <- XML::xmlParse("example-of-xml.xml")
htmlTextObject
htmlTextObject <- XML::htmlParse(htmlTextData)
htmlTextObject
htmlTextObject %>% html_nodes("p")
htmlTextObject %>% xml_find_all("p")
# 运行爬取脚本，在脚本中要将返回的html页面内容暴露为一个全局变量htmlText
reticulate::py_run_file("./parseHTML.py")
rootNode <- py$htmlText %>% read_html()
# rvest包中的这些函数挑选节点，使用css选择器和xpath两种方法之一，但不要同时使用
rootNode %>%
html_nodes(css = "h1.post-title") %>%
html_text() # 一路向量化操作
py$htmlText %>% str()
py$htmlText %>% class()
rootNode
# rvest包中的挑选节点函数，使用css选择器和xpath两种方法之一，但不要同时使用
rootNode %>%
html_nodes(css = "h1.post-title") %>%
html_text() # 一路向量化操作
# rvest包中的挑选节点函数，使用css选择器和xpath两种方法之一，但不要同时使用
rootNode %>%
html_nodes(css = "h1.post-title") %>%
html_text() %>% # 一路向量化操作
tibble()
# 1. 运行Python爬取脚本，在脚本中要将返回的html页面内容暴露为一个全局变量htmlText
reticulate::py_run_file("./parseHTML.py")
# 2. 用rvest::read_html()解析返回的网页（字符串）
rootNode <- py$htmlText %>% read_html()
# rvest包中的挑选节点函数，使用css选择器和xpath两种方法之一，但不要同时使用
rootNode %>%
html_nodes(css = "h1.post-title") %>%
html_text() %>% # 一路向量化操作
tibble() %>%
DT::datatable()
?setcolname
?setPackageName
?set_colnames
## global options
knitr::opts_chunk$set(
fig.width = 7, fig.asp = 0.618,
out.width = "90%", fig.align = "center",
fig.path = 'Figures/', fig.show = "hold",
warn = 1, warning = FALSE, message = FALSE,
echo = TRUE, comment = '#', collapse = F,
cache = T, cache.comments = F,
autodep = TRUE
)
## use necessary packages
library('pacman')
p_load(
# data processing
tidyverse, lubridate, data.table, magrittr,
# visualization
ggthemes, showtext, gridExtra,
# I/O
sqldf,
# web crawler
XML, rvest, httr, reticulate
)
## database engine
options(sqldf.driver = "SQLite")
# 1. 运行Python爬取脚本，在脚本中要将返回的html页面内容暴露为一个全局变量htmlText
reticulate::py_run_file("./parseHTML.py")
# 2. 用rvest::read_html()解析返回的网页（字符串）
rootNode <- py$htmlText %>% read_html()
# rvest包中的挑选节点函数，使用css选择器和xpath两种方法之一，但不要同时使用
rootNode %>%
html_nodes(css = "h1.post-title") %>%
html_text() %>% # 一路向量化操作
tibble() %>%
set_colnames("contents") %>%
set_rownames(NULL) %>%
DT::datatable()
# 1. 运行Python爬取脚本，在脚本中要将返回的html页面内容暴露为一个全局变量htmlText
reticulate::py_run_file("./parseHTML.py")
# 2. 用rvest::read_html()解析返回的网页（字符串）
rootNode <- py$htmlText %>% read_html()
# rvest包中的挑选节点函数，使用css选择器和xpath两种方法之一，但不要同时使用
rootNode %>%
html_nodes(css = "h1.post-title") %>%
html_text() %>% # 一路向量化操作
tibble() %>%
set_colnames("content") %>%
DT::datatable()
# 1. 运行Python爬取脚本，在脚本中要将返回的html页面内容暴露为一个全局变量htmlText
reticulate::py_run_file("./parseHTML.py")
# 2. 用rvest::read_html()解析返回的网页（字符串）
rootNode <- py$htmlText %>% read_html()
# rvest包中的挑选节点函数，使用css选择器和xpath两种方法之一，但不要同时使用
rootNode %>%
html_nodes(css = "h1.post-title") %>%
html_text() %>% # 一路向量化操作
tibble() %>%
set_colnames("content") %>%
set_rownames(NULL) %>%
DT::datatable()
# rvest包中的挑选节点函数，使用css选择器和xpath两种方法之一，但不要同时使用
content <- rootNode %>%
html_nodes(css = "h1.post-title") %>%
html_text() %>% # 一路向量化操作
tibble() %>%
set_colnames("content") %>%
set_rownames(NULL) %>%
DT::datatable()
# 1. 运行Python爬取脚本，在脚本中要将返回的html页面内容暴露为一个全局变量htmlText
reticulate::py_run_file("./parseHTML.py")
# 2. 用rvest::read_html()解析返回的网页（字符串）
rootNode <- py$htmlText %>% read_html()
# rvest包中的挑选节点函数，使用css选择器和xpath两种方法之一，但不要同时使用
content <- rootNode %>%
html_nodes(css = "h1.post-title") %>%
html_text() %>% # 一路向量化操作
tibble() %>%
set_colnames("content") %>%
set_rownames(NULL) %>%
DT::datatable()
# 1. 运行Python爬取脚本，在脚本中要将返回的html页面内容暴露为一个全局变量htmlText
reticulate::py_run_file("./parseHTML.py")
# 2. 用rvest::read_html()解析返回的网页（字符串）
rootNode <- py$htmlText %>% read_html()
# rvest包中的挑选节点函数，使用css选择器和xpath两种方法之一，但不要同时使用
content <- rootNode %>%
html_nodes(css = "h1.post-title") %>%
html_text() %>% # 一路向量化操作
tibble() %>%
set_colnames("content") %>%
set_rownames(NULL) %T>%
DT::datatable()
# 1. 运行Python爬取脚本，在脚本中要将返回的html页面内容暴露为一个全局变量htmlText
reticulate::py_run_file("./parseHTML.py")
# 2. 用rvest::read_html()解析返回的网页（字符串）
rootNode <- py$htmlText %>% read_html()
# rvest包中的挑选节点函数，使用css选择器和xpath两种方法之一，但不要同时使用
content <- rootNode %>%
html_nodes(css = "h1.post-title") %>%
html_text() %>% # 一路向量化操作
as.data.table() %>%
set_colnames("content") %>%
set_rownames(NULL)
content %>% DT::datatable()
# 1. 运行Python爬取脚本，在脚本中要将返回的html页面内容暴露为一个全局变量htmlText
reticulate::py_run_file("./parseHTML.py")
# 2. 用rvest::read_html()解析返回的网页（字符串）
rootNode <- py$htmlText %>% read_html()
# rvest包中的挑选节点函数，使用css选择器和xpath两种方法之一，但不要同时使用
content <- rootNode %>%
html_nodes(css = "h1.post-title") %>%
html_text() %>% # 一路向量化操作
as.data.table() %>%
set_colnames("content") %>%
set_rownames(NULL)
str(content)
content %>% DT::datatable()
# rvest包中的挑选节点函数，使用css选择器和xpath两种方法之一，但不要同时使用
content <- rootNode %>%
html_nodes(css = "h1.post-title") %>%
html_text() %>% # 一路向量化操作
as.data.table() %>%
set_colnames("content")
content[, id:=1:.N]
View(content)
# rvest包中的挑选节点函数，使用css选择器和xpath两种方法之一，但不要同时使用
content <- rootNode %>%
html_nodes(css = "h1.post-title") %>%
html_text() %>% # 一路向量化操作
as.data.table() %>%
set_colnames("content") %>%
mutate(id  = row_number())
View(content)
class(content)
# rvest包中的挑选节点函数，使用css选择器和xpath两种方法之一，但不要同时使用
content <- rootNode %>%
html_nodes(css = "h1.post-title") %>%
html_text() %>% # 一路向量化操作
as.data.table() %>%
set_colnames("content") %>%
mutate(id  = row_number()) %>%
select(id, content)
View(content)
# rvest包中的挑选节点函数，使用css选择器和xpath两种方法之一，但不要同时使用
content <- rootNode %>%
html_nodes(css = "h1.post-title") %>%
html_text() %>% # 一路向量化操作
as.data.table() %>%
set_colnames("title") %>%
mutate(id  = row_number()) %>%
select(id, content)
# 2. 用rvest::read_html()解析返回的网页（字符串）
rootNode <- py$htmlText %>% read_html()
# rvest包中的挑选节点函数，使用css选择器和xpath两种方法之一，但不要同时使用
content <- rootNode %>%
html_nodes(css = "h1.post-title") %>%
html_text() %>% # 一路向量化操作
set_colnames("title") %>%
mutate(id  = row_number()) %>%
select(id, content)
# rvest包中的挑选节点函数，使用css选择器和xpath两种方法之一，但不要同时使用
content <- rootNode %>%
html_nodes(css = "h1.post-title") %>%
html_text() %>% # 一路向量化操作
set_colnames("title") %>%
mutate(id  = row_number()) %>%
select(id, title)
# rvest包中的挑选节点函数，使用css选择器和xpath两种方法之一，但不要同时使用
content <- rootNode %>%
html_nodes(css = "h1.post-title") %>%
html_text() %>% # 一路向量化操作
set_colnames("title")
# rvest包中的挑选节点函数，使用css选择器和xpath两种方法之一，但不要同时使用
content <- rootNode %>%
html_nodes(css = "h1.post-title") %>%
html_text()
# rvest包中的挑选节点函数，使用css选择器和xpath两种方法之一，但不要同时使用
content <- rootNode %>%
html_nodes(css = "h1.post-title") %>%
html_text() %>% # 一路向量化操作
as.data.table() %>%
set_colnames("title") %>%
mutate(id  = row_number()) %>%
select(id, title)
View(content)
# rvest包中的挑选节点函数，使用css选择器和xpath两种方法之一，但不要同时使用
content <- rootNode %>%
html_nodes(css = "h1.post-title") %>%
html_text() %>% # 一路向量化操作
as.data.table() %>%
set_colnames("title") %>%
mutate(id  = row_number()) %>%
select(id, title)
content %>% DT::datatable()
content %>%
set_rownames(NULL) %>%
DT::datatable()
# rvest包中的挑选节点函数，使用css选择器和xpath两种方法之一，但不要同时使用
content <- rootNode %>%
html_nodes(css = "h1.post-title") %>%
html_text() %>% # 一路向量化操作
tibble() %>%
set_colnames("title") %>%
mutate(id  = row_number()) %>%
select(id, title)
content %>%
set_rownames(NULL) %>%
DT::datatable()
unlink('C:/Users/Humoonruc/OneDrive/ICT/Programming_Language/Python/crawler_course/唐松-2ed/myPractice/parseHTML_cache', recursive = TRUE)
?DT::datatable
# 1. 运行Python爬取脚本，在脚本中要将返回的html页面内容暴露为一个全局变量htmlText
reticulate::py_run_file("./parseHTML.py")
# 2. 用rvest::read_html()解析返回的网页（字符串）
rootNode <- py$htmlText %>% read_html()
# 3. rvest包中的挑选节点函数，使用css选择器和xpath两种方法之一，但不要同时使用
content <- rootNode %>%
html_nodes(css = "h1.post-title") %>%
html_text() %>% # 一路向量化操作
tibble() %>%
set_colnames("title") %>%
mutate(id  = row_number()) %>%
select(id, title)
content %>%
DT::datatable(rownames = F)
install.packages("httr")
install.packages("httr")
devtools::install_github("r-lib/httr")
devtools::install_github("r-lib/httr")
install.packages(c("boot", "cachem", "crayon", "MASS", "mime", "promises", "RcppArmadillo", "stopwords", "systemfonts", "testthat", "usethis", "waldo", "websocket", "xfun"))
library(httr)
library(httr)
headers = c("User-Agent" = 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Mobile Safari/537.36 Edg/88.0.705.63',
"Cookie" = "")
url = "www.baidu.com"
result <- GET(url, add_headers(.headers = headers))
result
source('~/.active-rstudio-document', echo=TRUE)
result[[1]]
result[1] %>% class()
library(tidyverse)
result[1] %>% class()
result["url"]
result["url"][[1]]
result
print(result)
result[["url"]]
result[["status_code"]]
n <- length(result)
n
for(i in 1:n){
result %>% extract(i) %>% print()
}
for(i in 1:n){
result %>% extract2(i) %>% print()
}
library(magrittr)
for(i in 1:n){
result %>% extract2(i) %>% print()
}
for(i in 1:n){
result %>% extract1(i) %>% print()
}
result %>% extract(i) %>% print()
for(i in 1:n){
result %>% extract(i) %>% print()
}
result[1]
library(httr)
headers = c("User-Agent" = 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Mobile Safari/537.36 Edg/88.0.705.63',
"Cookie" = "")
url = "www.baidu.com"
result <- GET(url, add_headers(.headers = headers))
result[1]
GET(url)[1]
result <- GET(url)
url = "www.baidu.com"
GET(url)
?GET
GET("http://google.com/")
url = "https://www.baidu.com"
GET(url)
headers = c("User-Agent" = 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Mobile Safari/537.36 Edg/88.0.705.63',
"Cookie" = "")
url = "https://www.baidu.com"
result <- GET(url, add_headers(.headers = headers))
result[1]
result[2]
result[3]
## global options
knitr::opts_chunk$set(
fig.width = 7, fig.asp = 0.618,
out.width = "90%", fig.align = "center",
fig.path = 'Figures/', fig.show = "hold",
warn = 1, warning = FALSE, message = FALSE,
echo = TRUE, comment = '#', collapse = F,
cache = F, cache.comments = F,
autodep = TRUE
)
## use necessary packages
library('pacman')
p_load(
# data processing
tidyverse, lubridate, data.table, magrittr,
# I/O
sqldf,
# web crawler
rvest, reticulate, httr
)
## database engine
options(sqldf.driver = "SQLite")
headers = c("User-Agent" = 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Mobile Safari/537.36 Edg/88.0.705.63',
"Cookie" = "")
url = "https://www.baidu.com"
result <- GET(url, add_headers(.headers = headers))
headers = c("User-Agent" = 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Mobile Safari/537.36 Edg/88.0.705.63',
"Cookie" = "")
url = "https://www.baidu.com"
result <- GET(url, add_headers(.headers = headers))
n <- length(result)
for (i in 1:n) {
result %>% extract(i) %>% print()
}
headers = c("User-Agent" = 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Mobile Safari/537.36 Edg/88.0.705.63',
"Cookie" = "")
url = "https://www.baidu.com"
result <- GET(url, add_headers(.headers = headers))
n <- length(result)
for (i in 1:n) {
str_c("result[", i, "] is", result %>% extract(i)) %>% print()
}
headers = c("User-Agent" = 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Mobile Safari/537.36 Edg/88.0.705.63',
"Cookie" = "")
url = "https://www.baidu.com"
result <- GET(url, add_headers(.headers = headers))
n <- length(result)
for (i in 1:n) {
str_c("result[", i, "] is:  ", result %>% extract(i)) %>% print()
}
headers = c("User-Agent" = 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Mobile Safari/537.36 Edg/88.0.705.63',
"Cookie" = "")
url = "https://www.baidu.com"
result <- GET(url, add_headers(.headers = headers))
n <- length(result)
for (i in 1:n) {
str_c("result[", i, "] is:\n", result %>% extract(i)) %>% print()
}
headers = c("User-Agent" = 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Mobile Safari/537.36 Edg/88.0.705.63',
"Cookie" = "")
url = "https://www.baidu.com"
result <- GET(url, add_headers(.headers = headers))
n <- length(result)
for (i in 1:n) {
str_c("result[", i, "] is:\\n", result %>% extract(i)) %>% print()
}
headers = c("User-Agent" = 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Mobile Safari/537.36 Edg/88.0.705.63',
"Cookie" = "")
url = "https://www.baidu.com"
result <- GET(url, add_headers(.headers = headers))
n <- length(result)
for (i in 1:n) {
(str_c("result[", i, "] is:\n", result %>% extract(i)))
}
blogdown:::preview_site()
blogdown:::preview_site()
blogdown:::preview_site()
blogdown:::preview_site()
blogdown:::preview_site()
blogdown:::preview_site()
blogdown:::preview_site()
blogdown:::preview_site()
blogdown:::preview_site()
blogdown:::preview_site()
blogdown:::preview_site()
blogdown:::preview_site()
