---
title: "小说中的人物关系网络图"
subtitle: ''
author: "黄蒙"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    fig_caption: yes
    theme: united
    highlight: haddock
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: true
      smooth_scroll: FALSE
  rticles::ctex:
    df_print: default
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: '2'
  word_document:
    fig_caption: yes
    toc: yes
    toc_depth: 3
    reference_docx: 
  pdf_document:
    toc: yes
    toc_depth: '2'
documentclass: ctexart
classoption: hyperref,
---

```{r setup, include = FALSE}
## global options
knitr::opts_chunk$set(
  fig.width = 7, fig.asp = 0.618,
  out.width = "90%", fig.align = "center",
  fig.path = "Figures/", fig.show = "hold",
  warn = 1, warning = FALSE, message = FALSE, echo = TRUE,
  comment = "", collapse = FALSE,
  cache = TRUE, cache.comments = FALSE, autodep = TRUE
)


## use necessary packages
library("pacman")
pacman::p_load(
  tidyverse, lubridate, data.table, # data processing
  ggthemes, showtext, gridExtra, igraph, ggraph, tidygraph, # visualization
  lmtest, plm, orcutt, stats, forecast, zoo, # 统计分析
  DT, # I/O
  jiebaR, wordcloud2, webshot, htmlwidgets, tidytext # 文本分析
)
options(sqldf.driver = "SQLite")


# 包含图的代码块需要 fig.showtext = TRUE选项
showtext_auto(enable = TRUE)
# ggplot2图形需要在主题中显式指定中文字体才能正常显示图中的中文
windowsFonts(H = windowsFont("Microsoft YaHei"))
## pdf输出中图形内部的中文字体设置
pdf.options(family = "GB1")


## 自定义一般图形主题
mytheme <- theme_economist_white() +
  theme(
    text = element_text(family = "Microsoft YaHei"),
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(
      hjust = 0, size = 10, margin = margin(2, 0, 0, 0, "pt")
    ),
    plot.margin = margin(12, 10, 12, 0, "pt"),
    legend.position = "top",
    legend.justification = "left",
    legend.margin = margin(4, 0, 0, 0, "pt"),
    legend.key.size = unit(1, "lines"),
    legend.title = element_text(size = 12),
    legend.text = element_text(
      size = 10, margin = margin(0, 0, 0, 0, "pt")
    ),
    axis.text = element_text(
      size = 10, margin = margin(2, 0, 2, 0, "pt")
    ),
    axis.ticks.length = unit(-4, "pt")
  )

# 自定义柱状图主题
theme_bar <- theme_economist_white() +
  theme(
    text = element_text(family = "Microsoft YaHei"), # 所有的文本字体
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(
      hjust = 0, size = 10,
      margin = margin(0, 0, 0, 0, "pt")
    ),
    plot.margin = margin(12, 0, 12, 10, "pt"),
    legend.position = "top",
    legend.justification = "left",
    legend.margin = margin(4, 0, 0, 0, "pt"),
    legend.key.size = unit(0.7, "lines"),
    legend.title = element_blank(),
    legend.text = element_text(
      size = 10, margin = margin(0, 8, 0, 4, "pt")
    ),
    axis.text = element_text(size = 8),
    axis.ticks.length = unit(0, "pt") # 不要坐标轴须
  )

# 自定义网络图主题
mytheme_graph <- theme_void() +
  theme(
    text = element_text(family = "Microsoft YaHei"), # 所有的文本字体
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 10),
    plot.caption = element_text(
      hjust = 0, size = 6,
      margin = margin(8, 0, 0, 0, "pt")
    ),
    plot.margin = margin(12, 0, 12, 10, "pt"),
    legend.position = "right",
    legend.justification = "left",
    legend.margin = margin(4, 0, 0, 0, "pt"),
    legend.key.size = unit(0.7, "lines"),
    legend.title = element_text(
      size = 7, margin = margin(0, 8, 0, 4, "pt")
    ),
    legend.text = element_text(
      size = 6, margin = margin(0, 8, 0, 4, "pt")
    )
  )
```


# 准备工作

```{r}
## 1. 拆分段落并编号
# readLines()读入文本为一个字符串向量，每一自然段为向量中的一个元素
# txt文件的编码模式，一般是"ANSI"，但也有个别的是"UTF-8"
total <- readLines("神雕侠侣.txt", encoding = "UTF-8") %>%
  str_trim() %>% # 去掉段首尾空格
  tibble(text = .) %>% # 将该向量转换为一个数据框
  filter(text != "") %>% # 去掉空行
  mutate(paragraph = row_number()) # 添加行号：段落编号


## 2. 合并人物称呼
# 读入人物名单
roles <- readLines("神雕侠侣人物名单.txt", encoding = "ANSI") %>%
  tibble(name = .) %>%
  filter(name != "") %>%
  pull(name)

# 利用正则表达式，对人物姓名和别称进行特定格式的处理，便于后面操作
roles_title <- str_c("(", str_replace_all(roles, " ", ")|("), ")") # 将空格替换为')|('，这一步是向量化操作
roles_name <- str_split_fixed(roles, " ", 2)[, 1] # 一个人可能有多种称呼，取其第一个称呼为姓名

# 针对一个自然段的替换函数
replace_name <- function(paragraph) {
  temp <- paragraph
  # 此处不能用向量化操作，因为不是同时操作n个对象
  # 而是操作一个对象（paragraph）并覆盖，迭代n次，所以用了显式循环
  for (i in 1:length(roles_name)) {
    if (roles[i] != roles_name[i]) { # 若相等，该人物只有一个称呼，就不用替换了
      temp <- str_replace_all(temp, roles_title[i], roles_name[i])
      # str_replace_all()的第一个参数为文本
      # 第二个参数为查找内容（用正则表达式表示）
      # 第三个参数为替换内容
      # 该语句将人物所有的别称替换为统一的姓名
    }
  }
  return(temp)
}

# 返回替换称呼后的作品文本，向量化操作
result <- map_chr(total %>% pull(text), replace_name)

# 替换原来的数据
replace_over <- total %>% mutate(text = result)

# 检测每一段中是否包含某个人物的函数，返回一个布尔列向量
if_in_para <- function(name) {
  result %>%
    str_detect(name) %>%
    return()
}
# 组合循环所得的若干布尔列向量，返回一个数据框
detect_matrix <- map_dfc(roles_name, if_in_para)
colnames(detect_matrix) <- roles_name # 添加列名
replace_over <- cbind(replace_over, detect_matrix)
```


# 文本分析

## 人物词频统计

```{r, fig.showtext = TRUE}

# 统计人物名出现次数的函数
count_name <- function(name) {
  replace_over %>%
    mutate(count = str_count(text, name)) %>%
    summarise(n = sum(count)) %>%
    pull(n) %>%
    return()
}

# 统计包含人物的段落数的函数
count_n <- function(name) {
  # 将数据框的列名作为参数传递给其他函数，要通过`!!sym()`
  replace_over %>%
    summarise(n = sum(!!sym(name))) %>% # 对布尔列向量求和，true为1，false为0
    pull(n) %>%
    return()
}
# 另一种写法
# count_n <- function(name){
# replace_over %>%
# mutate(count = str_detect(text, name)) %>%
# summarise(n = sum(count)) %>%
# pull(n) %>% return()
# }

# 统计包含人物的段落的总字数的函数
count_words <- function(name) {
  replace_over %>%
    filter(!!sym(name) == TRUE) %>%
    pull(text) %>%
    str_c(collapse = "\n") %>%
    str_length() %>%
    return()
}
# 另一种写法
# count_words <- function(name){
# replace_over %>%
# mutate(count = str_detect(text, name)) %>%
# filter(count == TRUE) %>%
# pull(text) %>%
# str_c(collapse = '\n') %>%
# str_length() %>%
# return()
# }

# 将上述三项统计结果汇总为一个数据框
role_freq <- tibble(
  name = roles_name,
  count = map(roles_name, count_name) %>% unlist(), # map()返回一个list，拆开才是一个向量
  n_para = map(roles_name, count_n) %>% unlist(),
  n_words = map(roles_name, count_words) %>% unlist()
) %>%
  arrange(desc(n_words)) # 按涉及字数排序

# 绘图
p1 <- role_freq %>%
  mutate(name = reorder(name, n_words)) %>% # 绘图按字数多少排序
  ggplot(aes(x = name, y = n_words)) +
  geom_bar(
    stat = "identity", position = "dodge",
    width = 0.7, fill = "#016392"
  ) +
  scale_y_continuous(position = "right") +
  labs(
    title = "人物出现段落的总字数",
    subtitle = "",
    x = "", y = ""
  ) +
  theme_bar
p1
ggsave(
  file = "./Figures/frequency.pdf", plot = p1,
  width = 8, height = 5
)
```

## 人物关系分析

```{r, fig.showtext = TRUE}

# 计算亲密度的函数，以共同出现段落的总字数为亲密度
intimate <- function(name1, name2) {

  # 选出共同出现的段落
  temp <- replace_over %>%
    filter(!!sym(name1) == TRUE & !!sym(name2) == TRUE)

  # 返回其总字数
  if (nrow(temp) == 0) {
    return(0)
  } else {
    temp %>%
      pull(text) %>%
      str_c(collapse = "\n") %>%
      str_length() %>%
      return()
  }
}

# 另一种计算亲密度的方法：共同出现的段落数
# intimate <- function(string1, string2){
# temp <- replace_over %>%
# mutate(count1 = str_detect(text, string1),
# count2 = str_detect(text, string2))
# temp$count1 %*% temp$count2 %>% return()
# }

# 构建亲密度矩阵
n <- length(roles_name)
intimate_matrix <- diag(rep(0, n))
colnames(intimate_matrix) <- roles_name
rownames(intimate_matrix) <- roles_name
for (i in 2:n) {
  for (j in 1:(i - 1)) {
    intimate_matrix[i, j] <- intimate(roles_name[i], roles_name[j])
  }
}
for (i in 1:(n - 1)) {
  for (j in (i + 1):n) {
    intimate_matrix[i, j] <- intimate_matrix[j, i]
  }
}

# 可视化
pheatmap::pheatmap(intimate_matrix, cluster_cols = F, cluster_rows = F)

# 记录其中最高值为全局变量
most_intimate <- max(intimate_matrix)
```

```{r, fig.showtext = TRUE}

## 将矩阵整理为graph的数据结构
relations <- tibble(from = "A", to = "B", intimate = 0)
for (i in 1:(n - 1)) {
  for (j in (i + 1):n) {
    temp <- tibble(
      from = roles_name[i], to = roles_name[j],
      intimate = intimate_matrix[i, j]
    )
    relations <- bind_rows(relations, temp)
  }
}
# 筛选亲密度在最高亲密度1/10以上的关系
relations <- relations %>%
  filter(intimate > most_intimate / 10)

# 选择亲密度足够大的节点，丢弃过于次要的人物
main_role <- c(relations$from, relations$to) %>%
  unique() %>%
  tibble(name = .)
role_freq <- semi_join(role_freq, main_role, by = "name") # 筛选主要人物节点

# 主要人物之间的亲密度矩阵
mainrole_intimate_matrix <- intimate_matrix[main_role$name, main_role$name]

# 可视化
pheatmap::pheatmap(mainrole_intimate_matrix,
  cluster_cols = T, cluster_rows = T
)



## 网络分析

# 生成igraph对象
graph <- graph_from_data_frame(
  relations,
  vertices = role_freq,
  directed = F
)

# 聚类分析
cfg <- cluster_fast_greedy(graph)
cfg
# plot(cfg, graph) # 图中中文无法正确显示

# 转换为tbl_graph类对象
graph_tg <- tidygraph::as_tbl_graph(graph) %>%
  mutate(deg = centrality_betweenness(normalized = T)) %>% # 增加中介中心度变量
  mutate(group = group_infomap()) # 增加节点群变量

# 绘制网络图
title_tg <- str_c(
  "人物关系网络图，集聚系数为 ",
  transitivity(graph_tg) %>% round(5)
)
set.seed(10)
p2 <- ggraph(graph_tg, layout = "kk") +
  # 若两个节点之间有多条边相连，geom_edge_fan()可以绘制多条连接它们的曲线
  geom_edge_fan(aes(edge_width = intimate),
    color = "lightblue",
    end_cap = circle(0.05, "inches"),
    show.legend = T
  ) +
  geom_node_point(aes(size = deg, fill = factor(group)),
    shape = 21
  ) + # 点的大小也可以映射为人物出现次数等指标，size = count/n_para/n_words
  geom_node_text(aes(label = name),
    size = 2.5,
    vjust = 1, hjust = 1
  ) +
  scale_color_discrete() +
  scale_edge_width(range = c(0.1, 2)) +
  guides(fill = F) +
  labs(
    title = title_tg,
    subtitle = "",
    size = "标准化的中介中心度",
    edge_width = "共同出现的段落字数",
    caption = ""
  ) +
  mytheme_graph
p2
ggsave(
  file = "./Figures/Network2.pdf", plot = p2,
  width = 8, height = 5
)


# 相关概念
transitivity(graph_tg) # 聚集系数，表示一个图形中节点的聚集程度，越是任意两个节点之间都有线相连的网络，聚集系数越高。
graph.density(graph_tg)
degree(graph_tg, normalized = T) # 节点中心度，在某个点上，有多少条线。
closeness(graph_tg, normalized = T) # 接近中心度，该点与网络中其他点距离之和的倒数，越大说明越在网络中心，越能够很快到达其他点。
betweenness(graph_tg, normalized = T) # 中介中心度，代表图中任意两点间最短距离是否都经过该点，如果都经过说明这个点很重要。这个指标更能说明转发、中介的情况。
```
